\section{Introduction}
\label{lIntroduction}
While the concept of the \gls{iot} has been around for about 20 years \cite{Hutchison.2010}, mainstream attention and adoption have just recently seen a steep increase. Between 2012 and 2017 the annual search interest on the \gls{iot} has increased more than nine-fold, according to data from Google (see Appendix \ref{lSearchInterestInIoT}). In the same timeframe, industrial adoption has been growing as well. According to an industry report commissioned by Microsoft \cite{Microsoft.2020}, 91\% of the contacted companies were in the process of adopting \gls{iot} technologies in 2019. In addition, 64\% of the companies reported that they plan to further expand their \gls{iot} ventures in the next two years.\par
In parallel with this rise in interest and industrial adoption, the number of connected devices is expected to more than quadruple between 2015 and 2025, reaching more than 75 billion at the end of 2025 \cite{B.Safaei.2017}. With this ever-growing number of connected devices, the significance of effectively deploying these devices in \gls{iot} applications is growing as well. For effective deployment, several requirements must be fulfilled. These prerequisites have been outlined by Yigitoglu et al. \cite{Yigitoglu.2017}. They are based on latency, bandwidth, resource constraints, mobility, dynamic workloads, multi-tenancy, and privacy. To deploy \gls{iot} devices and applications in line with these requirements, neither a solely cloud-based approach nor a solely edge-based concept is appropriate \cite{Yousefpour.2019}. Thus, a combined approach of edge and cloud computing is needed.\\
The fog computing paradigm addresses this combination, extending cloud computing to the network edge \cite{Bonomi.2012}. The capabilities and characteristics of edge computing (e.g., low latency, real-time computing, etc.) and cloud computing (e.g., abundance of computing resources, etc.) are combined in fog computing, making it a well-suited approach for most \gls{iot} applications \cite{Yousefpour.2019}.\\
Many tasks in the context of the \gls{iot} are continuously running and can be implemented in a sequence of steps. One way to address such a continuous task comprised of a sequence of steps is through stream processing. In stream-processing, an input data stream is processed in a sequence of actions, forming a so-called stream processing pipeline. Each of these actions is executed by an action specific processor, referred to as \gls{pe}. Additionally to the \gls{iot} context, stream processing is used in a variety of applications, including electronic trading, fraud control, and infrastructure monitoring \cite{Stonebraker.2005}.\\
Utilizing fog computing for stream processing can thus address \gls{iot} applications within an \gls{iot} compatible frame. However, the deployment of stream processing applications in the fog comes with some challenges. To visualize these challenges, two exemplary cases are introduced. These cases also illustrate the underlying technology, the addressed problems, and the developed conceptual framework in the subsequent chapters. While the first case is abstract and serves as an easy-to-follow example constructed for this thesis, the second case represents a real-world use case and is a simplified adoption for stream processing in the fog of the work of Chowdhurry et al. \cite{Chowdhury.2019}:\par

\begin{enumerate}[label=Case \arabic* , wide=0.5em,  leftmargin=*]
    \item \label{cAggregation} The first case is a simple example of a stream processing pipeline, which aggregates values. As depicted in Figure \ref{fExemplaryCases}, the stream processing pipeline gets its input data stream from a text file that contains numeric \gls{csv}. The second \gls{pe} aggregates incoming values over a predefined window of past events and passes on the result. In the last step, this data stream of values is written to an \gls{csv} output text file. Using this stream processing pipeline, the result's consistency can be monitored, and the functionality can be tested.
    \item \label{cMedical} The second case is based on a medical wearable device that monitors a patient’s electrocardiogram. Early symptoms of heart attacks are detected using the acquired electrocardiogram data. Therefore, the sensory data needs to be classified using a neural network, which can trigger an alarm on the patient’s wearable device. In detail, this application consists of 5 steps, as shown in Figure \ref{fExemplaryCases}. First, sensors acquire the data. Next, the signal is amplified, then it is filtered before the classification happens. Finally, an alarm on the wearable device is triggered when symptoms are detected.
\end{enumerate}

\begin{figure}[H]
\graphicspath{{./figures/code/}}
\includesvg[width=\textwidth]{figures/visualizations/ExemplaryCases_1500.svg}
\caption{Stream processing pipelines of the exemplary cases}
\label{fExemplaryCases}
\end{figure}


The application in \ref{cMedical} requires a notable amount of computing resources for the classification of the sensory information, while its overall computing time should be as small as possible to alarm the patient in time to take necessary actions. At the same time, the wearable device has only limited computational capabilities and cannot provide a sufficient service quality in the classification step. To cater to these needs, the this task is realized as a stream processing pipeline deployed in the fog. Fog computing is utilized by deploying the \gls{pe}s individually and distributed across fog nodes (different computing devices in the fog). To achieve a low latency the \gls{pe}s for the computational intensive steps are deployed on resourceful nodes with a low latency in the close vicinity. The \gls{pe} that inputs the electrocardiogram data into the stream processing pipeline, and the \gls{pe} that triggers the alarm are deployed on the wearable device. Due to wearable devices' mobile nature, the computing nodes in the close vicinity are constantly changing, thus requiring a possibility to relocate the pipeline's components while the pipeline is running. This dynamic relocation is referred to as workload-mobility \cite{Yigitoglu.2017}. Workload-mobility can also be important in the case of a resource scarcity at a node, bottlenecking the application's computing time, which would impose a significant toll on the usefulness of the application. Additionally, in a situation in which a fog node that hosts a \gls{pe} becomes unreachable, for example, because its network connection is interrupted, the \gls{pe} has to be relocated as well to facilitate the continuation of the pipeline's execution.\\
To address these challenges and, more broadly, all the previously introduced requirements outlined by Yigitoglu et al. \cite{Yigitoglu.2017}, it is necessary to adapt stream processing pipelines while they are running by relocating \gls{pe}s. This relocation is referred to as migration. Migration means that processing,  previously performed on a specific node, is shifted to another node, which takes over processing.\par
To address this issue, this thesis focuses on developing a conceptual framework for the migration of stream processing applications in the fog. More precisely, this thesis focuses on the following Research Questions (\acrshort{rq}):

\begin{enumerate}[label=RQ\arabic* , wide=0.5em,  leftmargin=*]
    \item \label{rqMigration} How can stateless and stateful event-driven \gls{pe}s be migrated between nodes in the fog?
    \item \label{rqConsistency} How can the data-stream's consistent processing be facilitated when an event-driven \gls{pe} is migrated between nodes in the fog?
\end{enumerate}

Addressing these research questions, this thesis fills gaps in existing research. To the best of the author's knowledge, there has not been any prior research into the migration of stateful stream processing applications that considered the fog computing paradigm's specific challenges and opportunities.\\
Therefore, this thesis addresses this topic by developing a conceptual framework for the migration of \gls{pe}s that also takes the specific challenges and opportunities of fog computing into account. Moreover, this thesis proposes separate migration tactics for different situations.\par
%Hier könnte ich mich auch alternativ von dem Teil zu der Contribution in Yigitoglu.2017 inspirieren lassen und meine Contribution ausführlich auflisten und den Ausblick dafür stark beschneiden 

After this brief introduction and motivation, the second chapter investigates the enabling technologies for this thesis, giving a high-level overview of fog computing and stream processing, emphasizing the role of state in stream processing. Thereby, chapter \ref{lBackground} lays the groundwork for the subsequent chapters. The third chapter introduces related work. It contemplates related works that assess the migration of applications in the fog and works that concern state in data processing. These works propose different approaches for the migration of applications and for state management. In chapter \ref{lMethodology} these approaches approaches are compared and the requirements for the migration of stream processing \gls{pe}s in the fog are outlined.This chapter's main focus, however, is on the development of a conceptual framework for the migration of \gls{pe}s. The developed conceptual framework handles the migration of running and interrupted \gls{pe}s distinctly. For running \gls{pe}s \textit{\acrshort{pe} live migration} is proposed, interrupted \gls{pe}s are relocated using the \textit{\acrshort{pe} restoration}. The \textit{\acrshort{pe} restoration} relies on globally available \gls{pe} state checkpoints. To acquire these checkpoints dual-level asynchronous checkpointing is introduced. As proof of concept, the developed conceptual migration framework is then implemented into Apache StreamPipes (incubating) in chapter \ref{lImplementation}. This chapter provides an in-depth overview of the concept's implementation and outlines implementation details of modifications and extensions of StreamPipes, presupposed by the conceptual framework. In chapter \ref{lEvaluation}, the proof of concept implementation is evaluated in several experiments to assess whether and to which extent the requirements for the conceptual framework are met. The chapter is closed by pointing out the limitations of the thesis. Finally, the last chapter summarizes the previous chapters' key points while outlining possible directions for further research complementing this thesis.
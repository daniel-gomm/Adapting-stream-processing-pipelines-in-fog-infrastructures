\section{Background}
\label{lBackground}
This chapter introduces fog computing and stream processing. A profound understanding of both of these topics is needed as background for the subsequent chapters. Thus, fog computing is thoroughly introduced, defined, and its intricacies are pointed out. Furthermore, stream processing is introduced together with the concept of state in the stream processing context. State is then examined in detail, and relevant aspects in the handling of state are pointed out.

\subsection{Fog Computing}
\label{lFogComputing}
There are various definitions of the fog computing paradigm. In this thesis, the definition of Bonomi et al. \cite{Bonomi.2012} is used. They state that "fog computing extends the cloud computing paradigm to the edge of the network" \cite{Bonomi.2012}. They further describe fog computing as a highly virtualized platform that connects end devices with traditional cloud services by providing compute, storage, and networking services in between.\\
This definition of fog computing includes all available connected devices, meaning that compared to cloud computing, which typically solely utilizes large data centers \cite{Yousefpour.2019}, fog computing takes advantage of small servers, routers, switches, gateways, set-top boxes, and other small computing devices \cite{Yousefpour.2019} as well as the cloud infrastructure, which remains a core component.\par
To give a more comprehensive overview the characteristics that portray fog computing are presented in the following listing: 

\begin{itemize}
    \item Since a lot of the devices used in fog computing occupy a relatively small space, this hardware can be located close to the network edge \cite{Yousefpour.2019}, allowing for \textbf{low latency} and \textbf{location aware computing} \cite{Bonomi.2012, Yigitoglu.2017}. In \ref{cMedical}, hardware close to the edge and location-aware computing is used to allow a low latency.
    \item The fog is comprised of many different devices with vastly varying capabilities. This makes the \textbf{heterogeneity of devices} \cite{Bonomi.2012} another characteristic of fog computing. For example, in \ref{cMedical}, the pipeline is deployed on a wearable device with limited computational capabilities and devices in the close vicinity that possess significantly more computational capabilities. These edge near devices host the more computationally intensive \gls{pe}s.
    \item A \textbf{large number of nodes} makes \textbf{privacy} a priority \cite{Yigitoglu.2017} since a large amount of data is collected (especially in the \gls{iot}) and after that distributed for processing, requiring security to be provided at the edge to ensure that sensitive data is not shared with unwanted resources \cite{Yigitoglu.2017}. At the same time, fog computing can enhance privacy. For example, production data in an industrial \gls{iot} application can be preprocessed in the company's local network before sending it to cloud servers performing the rest of the computation.
    \item \textbf{Interoperability} between fog nodes is needed since many applications are deployed across multiple fog nodes \cite{Bonomi.2012}. This is especially prevalent in stream processing applications since when a stream processing pipeline is deployed across multiple fog nodes, their correct interplay is crucial for a consistent operation.
    \item Devices in the fog are usually \textbf{geographically distributed} \cite{Bonomi.2012} and connected to the network predominantly through wireless \textbf{access} \cite{Osanaiye.2017, Bonomi.2012, Baccarelli.2018}. The geographic distribution is exploited in \ref{cMedical} to lower the computation time by only deploying \gls{pe}s on fog nodes in the close vicinity that have a low latency.
\end{itemize}

As presented, the heterogeneity of devices is a characteristic of fog computing. With the heterogeneity of devices comes a heterogeneity of resources, architectures, and operating systems \cite{Puliafito.2019}. Addressing this issue, virtualization technologies are utilized to provide a more generic environment for applications on fog nodes \cite{Puliafito.2019}. Hence, it is necessary to look at commonly deployed virtualization technologies.\\
The two main approaches taken for virtualization are hardware virtualization in the form of so-called \gls{vm} and operating system level virtualization, better known as containerization \cite{Puliafito.2019}. Sharing the kernel with the operating system makes containerization more lightweight than virtual machines, which leads to containerization being the more appropriate solution in regards to the overall footprint and support of workload mobility \cite{Puliafito.2019}. Containerization is particularly vital in fog computing since processing is not only performed on devices with an abundance of computing resources, but also on smaller devices that only provide moderate computing resources at low power consumption \cite{Jalali.2016}. In many cases, this makes it difficult or even impossible to deploy virtual machines. The choice between \gls{vm}s and containers is context and need dependent, but due to the described factors, containerization has become the reference technology for fog computing in the last years \cite{Puliafito.2018}.\par

Fog computing is used in a multitude of different use cases \cite{Yousefpour.2019}, mostly in combination with \gls{iot} devices. These use cases include connected vehicles, health care, smart city/smart grid, video surveillance, and industrial \gls{iot} \cite{Yousefpour.2019}. In these cases, using fog computing can address volatile workloads, resource constraints, latency requirements, and privacy concerns \cite{Yigitoglu.2017}.


\subsection{Stream Processing}
\label{lStreamProcessing}
Stream processing links a collection of parallelly computing \gls{pe}s through channels \cite{Stephens.1997}. These systems of linked \gls{pe}s are called stream processing systems \cite{Stephens.1997} or stream processing pipelines. In stream processing pipelines, data is passed in between \gls{pe}s in the form of a data stream, which is a list of distinct data elements sourced from a data set of interest with indefinite length \cite{Stephens.1997}. The data in a stream could, for example, be stock prices, postings in social networks, or sensory data \cite{Hesse.2015} like the electrocardiogram data in \ref{cMedical}.\\
In stream processing systems there are three typically differentiated types of \gls{pe}s. This differentiation is based on their in- and output configuration. Distinguished are:
\begin{itemize}
    \item \gls{pe}s that have no inputs. They are called sources \cite{Stephens.1997} and pass data into the system where it is further processed. In \ref{cMedical}, the source is the \gls{pe} that inputs the sensory electrocardiogram data into the system.
    \item Filters (also called agents or processors) that take input data, perform computation on it and output the result \cite{Stephens.1997}. Examples for these intermediary processors are the amplifying, filtering, and classifying \gls{pe}s from \ref{cMedical}.
    \item \gls{pe}s that only possess inputs, which are named sinks \cite{Stephens.1997}. The corresponding \gls{pe} from \ref{cMedical} is the \gls{pe} that alarms the patient.
\end{itemize}

Filters can be further categorised into deterministic and non-deterministic filters \cite{Stephens.1997}. While deterministic filters follow a functional specification that produces a predictable outcome, non-deterministic filters do not have a predictable outcome.\\
Furthermore \gls{pe}s can be categorized into stateful and stateless \gls{pe}s \cite{CastroFernandez.2013}. The difference between these types of \gls{pe}s is extensively discussed in chapter \ref{lStateDataProcessing}.

\subsubsection{Event-Driven Stream Processing}
\label{lEventDrivenStreamProcessing}

An event-driven stream processing pipeline is a particular form of a stream processing pipeline. It consists of a number of event-driven \gls{pe}s, which operate on special data elements that are called events. In principle, every notable thing can be an event \cite{Michelson.2006}. An event consists of an event header and an event body \cite{Michelson.2006}. While the event header describes the occurrence of the element (with a timestamp, event creator, etc.), the event body represents what happened \cite{Michelson.2006}.\\
Looking at \ref{cMedical}, an event passed into the pipeline by the source \gls{pe} might look like the illustration in Figure \ref{fMedicalEvent}.\par

\begin{figure}[H]
    \centering
    \graphicspath{{./figures/code/}}
\includesvg[width=5.2cm]{figures/visualizations/MedicalEvent_500.svg}
\caption{Exemplary visualisation of an event}
\label{fMedicalEvent}
\end{figure}

Generally, in a system that follows an event-driven architecture the occurrence of an event entails an action \cite{Michelson.2006}. In event-driven stream processing this action usually is the processing of the event in an event-driven pipeline consisting of event-driven \gls{pe}s. This structure of event-driven components facilitates the loose coupling, and distribution of components \cite{Michelson.2006}. Event-driven \gls{pe}s thus act as reusable, independent event-driven functions.\\
The following chapters address event-driven stream processing. For a better readability the event-driven \gls{pe}s, and stream processing pipelines are referred to as \gls{pe}s, and pipelines from here on.


\subsubsection{State in Stream Processing}
\label{lStateStreamProcessing}

One major categorization for \gls{pe}s is whether they have a state and are therefore stateful or if they are stateless \cite{CastroFernandez.2013}. Usually, this differentiation refers to \gls{pe}s whose outputs solely depend on the current event as stateless and to the \gls{pe}s whose outputs additionally depend on past events as stateful. The concept of state is assessed to more precisely differentiate these categories of \gls{pe}s.\par

To et al. \cite{To.2017} define the state as "the intermediate value of a specific computation that will be used in subsequent operations during the processing of a data flow".  When considering stream processing, each \gls{pe} can have its own state. The most commonly used abstraction of state is the so-called operator state \cite{To.2017}, which describes the state from the \gls{pe}'s point of view \cite{To.2017}. The operator state consists of the processing, buffer, and routing state \cite{To.2017}.
The \textit{processing state} is present in all \gls{pe}s, whose output depends on the current input and the past input \cite{CastroFernandez.2013}. It is composed of a set of values that have been obtained through past computation. To interact with the processing state from an outside perspective, it has to be exposed. Exposing the processing state can be achieved by storing it in mutable data structures \cite{CastroFernandez.2016}, or by providing an interface to in-memory processing state.\\
It is a common practice to keep output buffers between \gls{pe}s in stream processing to compensate for temporary network and stream rate problems\cite{CastroFernandez.2013}. The \textit{buffer state} accounts for the events which have been processed by a \gls{pe} but have yet to be processed by the ensuing \gls{pe} \cite{CastroFernandez.2013}. In this thesis, the concept of buffer state is extended to not only account for the output buffers, but also for the positional information about the event that has most recently been processed by the \gls{pe}. Thus, the positional information represents how far processing has progressed in the event-stream, linking the operator state to an exact point in event-processing.\\
Finally, the \textit{routing state} holds the information on how the \gls{pe}'s output events should be routed to the following \gls{pe}s \cite{CastroFernandez.2013}. The routing state bears particular importance when the output destination can change at runtime.\par
Using the abstraction of operator state, so-called stateless \gls{pe}s do possess a buffer and a routing state but lack a processing state. Only the so-called stateful \gls{pe}s possess a processing state as well. This means that using the abstraction of state as operator state, the terms stateful and stateless specifically refer to the processing state.\par

\textbf{Management of State}\par
The effective management of state requires a multitude of operations on the state \cite{To.2017}. In the following, a few particularly important operations are described.\\
A key aspect of state handling revolves around the question of how to \textit{store and purge state} effectively. State can be stored either in-memory or in persistent storage \cite{To.2017}. While storing in-memory yields performance improvements \cite{Venkatesh.2019}, the stored state is not persistent and therefore does not survive system restarts. Somewhat of a middle way is taken by Apache Flink \cite{.06082020}, when using RocksDB as so-called "State Backend", since RocksDB \cite{GitHub.06082020} inserts new writes into an in-memory data structure, which is flushed to a file on storage when it is filled up.\\
To prevent an expired state from taking up storage capacity, the data can be purged \cite{To.2017}, which is another operation in state management that can be implemented into a system.\\
The \textit{migration} of the \gls{pe}'s state is an operation that is essential to the migration of the respective \gls{pe}. This operation can be implemented in various ways with different strengths and weaknesses. The following chapter discusses these different tactics in greater detail.\par

\textbf{Checkpointing State}\par
In the context of stateful stream processing, a checkpoint refers to a backed-up intermediate result of a \gls{pe}, from which processing can be recovered \cite{Zheng.20}. The checkpoint describes the state of the checkpointed entity at a specific point in time.\par
The checkpointing process can either be coordinated globally across the whole pipeline, or independently for every \gls{pe} \cite{Gibson.2014}. Secondly, it can be distinguished whether checkpointing is performed synchronously or asynchronously \cite{Gibson.2014}. These categorizations result in the following commonly used checkpointing tactics:

\begin{itemize}
    \item In \textbf{Synchronous global checkpointing}, a consistent snapshot of all \gls{pe}s involved in the pipeline is obtained. The checkpoint contains the individual states of the affected \gls{pe}s, all representing the same point in time. This can be achieved, for example, by using a “stop-the-world” approach \cite{Murray.2013, Gibson.2014}, where incoming data is stopped, remaining data is processed, and after that, the states of all involved entities are captured.
    \item \textbf{Asynchronous global checkpointing} also achieves a snapshot across all involved nodes, but it allows checkpointing of nodes at different times \cite{Gibson.2014}. A famous example of this process is the Chandy-Lamport algorithm \cite{Chandy.1985}. The thereby acquired checkpoints must, in addition to the \gls{pe} state checkpoints, account for the processing related state changes that happened during checkpointing \cite{Chandy.1985}.
    \item In \textbf{asynchronous independent checkpointing} the state of single \gls{pe}s is captured independently of other \gls{pe}s from the same pipeline. Thus, the checkpoints of the \gls{pe}s comprising the pipeline do not necessarily provide a consistent global state \cite{Gibson.2014}.
\end{itemize}



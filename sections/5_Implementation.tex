\section{Implementation}
\label{lImplementation}

To show the validity of the developed conceptual framework, the \textit{\acrshort{pe} live migration}, the \textit{\acrshort{pe} restoration}, and the supplementary concept element of \gls{dlac} are implemented in this chapter. This implementation serves as a proof of concept. The implementation is realized as an extension of the open-source data analytics platform Apache StreamPipes (incubating). The beginning of this section gives an introduction to StreamPipes and its intricacies. After that, the modifications to StreamPipes, necessary to enable migration, are presented. Closing out this chapter, the implementation details of the concept elements developed in chapter \ref{lMethodology} are outlined. 

\subsection{StreamPipes}
\label{lStreamPipes}

\begin{figure}[!b]
    \centering
    \graphicspath{{./figures/code/}}
    \includesvg[width=13.87cm]{figures/visualizations/StreamPipesComponents_1387.svg}
    \caption{Components of the StreamPipes application}
    \label{fStreamPipesSetup}
\end{figure}

Apache StreamPipes (incubating) provides a self-service toolbox for the analysis of data streams. The target users are domain experts, who do not necessarily possess programming experience. For this reason, StreamPipes offers a \gls{gui} through which domain experts can create stream processing pipelines themselves. StreamPipes is mainly deployed in the (industrial) \gls{iot} and is extendable through an openly available software development kit that provides templates for different types of \gls{pe}s.\\
The components that comprise StreamPipes are depicted in Figure \ref{fStreamPipesSetup}. All components are deployed in an interconnected web of docker containers. Docker is a frequently used containerization software in fog computing \cite{Brogi.2018}. The main component of the StreamPipes application is the backend, which orchestrates and oversees the lifecycle of deployed pipelines. It works in collaboration with separately set-up microservices, which offer extending functionalities, for example, store pipeline and user information. Users interact with the backend indirectly through the \gls{gui}, which is deployed as a web application. The \gls{gui} uses the backend's \gls{rest} endpoints as an interface to its functionalities. Users create stream processing pipelines through the so-called pipeline editor, which is part of the \gls{gui}. StreamPipes realizes pipelines as an interconnected sequence of \gls{pe}s.\\
The \gls{pe}s are implemented as reusable, and independent event-driven functions, meaning that they can take events as input, perform a specific task on them, and/or output events. The \gls{pe}s are further categorized into \textit{data streams} feeding event streams into the pipeline, intermediary \gls{pe}s called \textit{processors} transforming one or more input event streams to an output event stream, and \gls{pe}s named \textit{sinks}, which solely take inputs but do not produce outputs. The StreamPipes \gls{sdk} provides wrappers to implement \gls{pe}s in different programming languages (e.g., Java, Python) and for Big Data processing engines like Apache Flink. The proof of concept implementation only addresses the \gls{pe}s implemented using the java wrapper since .\\
The interconnections between sequential \gls{pe}s are handled by intermediary message brokers like Apache Kafka or Apache ActiveMQ. While it is possible to implement the conceptual framework for all types of message brokers, Apache Kafka is used in this thesis because its native message retention capability and offset management enable a convenient way to manage buffer state.

\subsection{Extension of StreamPipes}
\label{lModifications}

To implement the conceptual framework into StreamPipes, the existing application has to be modified and extended. First, the \gls{pe} framework is modified to allow operator state management. Second, the supplementary checkpointing concept \gls{dlac} is implemented.

\subsubsection{Integrating Operator State}
\label{lIntegratingOperatorState}

As discussed in chapter \ref{lMethodology}, the \gls{pe} needs to provide and handle the buffer state and the processing state information.\par

\textbf{Processing State}\par

\begin{figure}[!t]
    \centering
    \tikzumlset{fill class=backcolour}
    \begin{tikzpicture}
    
    \umlinterface[]{PipelineElement}{}{
    onDetach() : void
    }
    \umlinterface[x=-4.5, y=-4]{EventProcessor}{}{
    onInvocation(B, SpOutputCollector, 
    \\EventProcessingRuntimeContext): void \\ onEvent(Event, SpOutputCollector) : void
    }
    \umlinterface[x=4.5, y=-4]{EventSink}{}{
    onInvocation(B, \\EventSinkRuntimeContext): void \\ onEvent(Event, SpOutputCollector) : void
    }
    
    \umlclass[type=abstract,x=-4.5,y=-8.5]{StatefulEventProcessor}{\# stateHandler : StateHandler}{
    + getState() : String \\ + setState(String) : void
    }
    
    \umlclass[type=abstract,x=4.5,y=-8.5]{StatefulEventSink}{\# stateHandler : StateHandler}{
    + getState() : String \\ + setState(String) : void
    }
    
    \umlinherit[geometry=|-]{EventProcessor}{PipelineElement}
    \umlinherit[geometry=|-]{EventSink}{PipelineElement}
    \umlimpl[geometry=-|]{StatefulEventProcessor}{EventProcessor}
    \umlimpl[geometry=-|]{StatefulEventSink}{EventSink}
    
    \end{tikzpicture}
    
    
    \caption{UML class diagram for the classes \textit{StatefulEventProcessor} and \textit{StatefulEventSink}}
    \label{fStatufulEventOperatorUML}
\end{figure}
To add a new java \gls{pe} to StreamPipes, developers using the java \gls{pe} wrapper must implement a set of classes. The class containing the event processing logic (processor class) must either implement the \textit{EventProcessor} or the \textit{EventSink} interface, depending on what kind of \gls{pe} is designed. These interfaces do not expose the processing state, which means that the processing state cannot be migrated without an extension. To facilitate manageable processing state in \gls{pe}s, the abstract classes \textit{StatefulEventProcessor} and \textit{StatefulEventSink} were added, as shown in Figure \ref{fStatufulEventOperatorUML}. These newly introduced classes add the methods "getState" and "setState", which act as interfaces to the \gls{pe}'s processing state.\\
Using this extension, a stateful \gls{pe} is implemented with a processor class that extends either the \textit{StatefulEventProcessor} or the \textit{StatfulEventSink} class. From thereon, developers can either choose to use the provided default processing state handling or opt to override the "getState" and "setState" methods. Developers might choose to implement a customized processing state handling, if it yields performance improvements or if the default state handling struggles to serialize or restore the processing state consistently.\\
This extension introduces the differentiation between stateful and stateless \gls{pe}s and allows access to the processing state of stateful \gls{pe}s. Additionally, existing \gls{pe}s that implement the \textit{EventProcessor} or \textit{EventSink} interface can still be used without any adaptation. However, if they possess a processing state, they should be updated to extend one of the introduced abstract classes instead and to expose their processing state accordingly.\par

\begin{figure}[H]
    \begin{lstlisting}[language=Java]
    public class Counter extends StatefulEventProcessor<CounterParameters> {
    @StateObject public int counter = 0;
    
    @Override
    public void onInvocation(CounterParameters parameters, SpOutputCollector spOutputCollector, EventProcessorRuntimeContext runtimeContext) { 
    this.stateHandler = new StateHandler(this);
    }
    @Override
    public void onEvent(Event event, SpOutputCollector out) {
        event.addField("aggregation", ++counter);
        out.collect(event);
    }
    @Override
    public void onDetach() {}
    }
    \end{lstlisting}
    \caption{Implementation of a \gls{pe} that counts the events it has processed}
    \label{fCounterImplementation}
\end{figure}


The default processing state handling is coordinated by a \textit{StateHandler} entity. This \textit{StateHandler} entity has to be initialized in the obligatory "onInvocation" method implemented in the \gls{pe}. The \textit{StateHandler} serializes and restores the \gls{pe}'s processing state. It uses a pluggable serializer (default: Gson-serializer) to serialize and deserialize the processing state. Developers, who use this default processing state handling for their \gls{pe}, need to mark all member variables that comprise the processing state with an "@StateObject" annotation, as illustrated in Figure \ref{fCounterImplementation}. The member variables that compose the processing state are acquired by the \textit{StateHandler} through the Java Reflection API.\\
By offering the \textit{StateHandler}, and the possibility to provide custom functions that expose the processing state, the benefits of both approaches can be combined. On the one hand, using the provided \textit{StateHandler} makes exposing the processing state easily implementable, which addresses the \ref{rDevelopment} requirement. On the other hand, implementing \gls{pe}-specific processing state getter and setter functions can help to address \gls{pe}-specific particularities. For example, \gls{pe}-specific possibilities for performance improvements can be seized, which is beneficial regarding the \ref{rDowntime} requirement. Dealing with consistency issues is also facilitated, addressing the \ref{rConsistency} requirement.\par


\textbf{Buffer State}\par


The buffer state is obtained from the connection to the intermediary Apache Kafka message broker. Apache Kafka is a distributed event streaming platform, which offers a publish-subscribe based messaging system \cite{apacheKafka.2020}. This system works with events as messages, which are organized and stored in so-called topics \cite{apacheKafka.2020}. Each event in a topic gets assigned an offset, which refers to the event's position in the overall order of events. Additionally, Kafka keeps tracks of which events have already been successfully processed and which have not.\\
\begin{figure}[!b]
    \centering
    \tikzumlset{fill class=backcolour}
    \begin{tikzpicture}
    
    \umlinterface[]{EventConsumer}{...}{
    connect(TP, InternalEventProcessor<byte[]> : void \\ disconnect() : void \\ getConsumerState() : String \\ setConsumerState(String) : void \\ pause() : void \\ resume() : void \\ ...
    }
    
    \umlclass[y= -8]{SpKafkaConsumer}{...}{
    + connect(KafkaTransportProtocol, InternalEventProcessor<byte[]> : void \\ + disconnect() : void \\ + isConnected() : Boolean \\ + getConsumerState() : String \\ + setConsumerState(String) : void \\ + pause() : void \\ + resume() : void \\ + isPaused() : boolean \\ + run() : void \\ ...
    }
    \umlimpl[geometry=-|]{SpKafkaConsumer}{EventConsumer}
    
    \end{tikzpicture}
    \caption{Abbreviated UML class diagram for the \textit{SpKafkaConsumer}}
    \label{fKafkaConsumerUML}
\end{figure}
The \gls{pe} connects to the message broker through \textit{EventConsumer}s, which fetch data from a message broker, and \textit{EventProducer}s, which publish events to a message broker. The \textit{EventConsumer} interface is expanded with a “getConsumerState” method and a “setConsumerState” method, which expose the buffer state. These methods are implemented in the \textit{SpKafkaConsumer} class as shown in Figure \ref{fKafkaConsumerUML}. Additionally, the methods "pause" and "resume" are implemented. When "pause" is executed, event processing is suspended until the "resume" function is triggered. This temporary suspension of event processing is not only needed in the \textit{\acrshort{pe} live migration} but also every time the operator state is serialized. Pausing the event consumption during \gls{pe} state serialization enables a consistent serialization of the entire operator state.\\
For the representation of the buffer state, Apache Kafka's offset management is used. Thus, the serialized buffer state contains the consumer's current offsets and its group Id, which represent the exact position in the data stream when the serialized state is requested. When the buffer state is set, this information is deserialized and is used in the consumer's connection procedure, during which it connects to the Kafka message broker. This connection procedure was modified to enable processing pick-up and state-restoration. Algorithm \ref{aRestore} presents the newly introduced state restoration procedure as simplified pseudo-code. It replaces the previous connection process, which did not allow reconnecting to a specific offset. In line \ref{ifOffset}, the algorithm checks whether the provided offset, from which processing should be picked up, is unequal to the latest offset stored by Kafka. If this is untrue, processing can start as usual, because the provided offset already corresponds with the latest offset. If it is true, events were processed after the provided snapshot was taken, signifying that events must be reprocessed (as depicted in the last chapter in Figure \ref{fTimingOSR}). As already mentioned, reprocessing refers to replaying past events to recreate the correct processing state, from which regular processing can continue. Reprocessing is performed in line \ref{reprocess}. In contrast to regular processing, reprocessing does not produce any output. When all events until the latest offset have been reprocessed, the loop is interrupted (line \ref{ifBreak} and \ref{whileSmaller}), and regular processing starts from the latestOffset onwards.\par


\begin{algorithm}[H]
\alglanguage{pseudocode}
\caption{Consumer restoration algorithm}\label{aRestore}
\begin{algorithmic}[1]
\Procedure{RestoreConsumer}{$offset$}
\State KafkaConsumer.subscribe($topic$)
\State $latestOffset \gets KafkaConsumer.currentOffset$
\If{$offset \ne latestOffset$}\label{ifOffset}
    \State KafkaConsumer.setOffsetTo($offset$)
    \While{$KafkaConsumer.currentOffset < latestOffset$}\label{whileSmaller}
        \State $events \gets KafkaConsumer.pollEvents()$
        \For{\textbf{each} $event$ \textbf{in} $events$}
            \If{$event.offset \geq latestOffset$}\label{ifBreak}
                \State \textbf{break}
            \EndIf
            \State engine.reprocess($event$)\label{reprocess}
        \EndFor
    \EndWhile\label{restoreendwhile}
    \State KafkaConsumer.setOffsetTo($latestOffset$)
\EndIf\label{restoreendif}
\EndProcedure
\end{algorithmic}
\end{algorithm}






\subsubsection{Dual Level Asynchronous Checkpointing}
\label{lImplementationDLAC}

\begin{figure}[!b]
    \centering
    \graphicspath{{./figures/code/}}
    \includesvg[width=15.09cm]{figures/visualizations/StateDatabaseSetup_1509.svg}
    \caption{Setup of the StateDatabase used for \gls{dlac}}
    \label{fStateDatabaseSetup}
\end{figure}

The foundation for \gls{dlac} is the state-database, which stores checkpoints of \gls{pe} states. This database is implemented using the java wrapper for RocksDB named RocksJava. RocksDB is a persistent key-value store that offers high performance, especially on flash storage \cite{zhichaoCao.2020}. The setup of the state-database is presented in Figure \ref{fStateDatabaseSetup}. \gls{pe}s are registered in the state-database through the \textit{DatabasesSingleton}. Each registered \gls{pe} has a corresponding instance of a \textit{PipelineElementDatabase}, which acts as an interface to a column family in the \textit{StateDatabase}. Column families are logical partitions in RocksDB, which can be thought of like columns for key-value pairs. Each key-value pair is affiliated with exactly one column family. The \textit{StateDatabase} assigns a unique column family to each registered \textit{PipelineElementDatabase}. Using this implementation, each \gls{pe} has its own corresponding column family inside one database. The \textit{StateDatabase} uses RocksJava to persist checkpoints to storage. It uses a singleton design pattern to facilitate the setup of a single database with multiple column families. When a new \textit{PipelineElementDatabase} is instantiated, it registers itself in the \textit{StateDatabase} and gets assigned its unique \textit{ColumnFamilyHandle}, which is needed to save data to and read data from a column family. In short, the \textit{StateDatabase} sets up \gls{pe}-specific column families, which are interfaced to through \textit{PipelineElementDatabases}. All \textit{PipelineElementDatabases} are tracked by the \textit{DatabasesSingleton}.\\
This setup has the advantage of only requiring a single RocksDB database, regardless of the number of \gls{pe}s whose checkpoints are stored. The checkpoints from different \gls{pe}s are stored in \gls{pe}-specific column families. Each \textit{PipelineElementDatabase} instance provides an encapsulation of one column family. By assigning exactly one \textit{PipelineElementDatabase} to one \gls{pe}, the assignment of column families to \gls{pe}s becomes definite.\par

\begin{figure}[!b]
    \centering
    \tikzumlset{fill class=backcolour}
    \begin{tikzpicture}
    
    \umlinterface[x=9, y=1]{Runnable}{}{
    run() : void
    }
    \umlinterface[x=9, y=-4]{CheckpointingWorker}{...}{
    startWorker() : void \\ stopWorker() : void
    }
    
    \umlclass[type=enum,y=0,x=0]{ContainerCheckpointingWorker}{- invocations : TreeMap<Long,ContainerTrackedDatabase> \\ ...}{
    + registerPipelineElement(InvocableDeclarer, String) : void \\ + unregisterPipelineElement(String) : void \\ + run() : void \\ ...
    }
    
    \umlclass[type=enum,y=-8,x=0]{BackendCheckpointingWorker}{- invocations : TreeMap<Long,BackendTrackedDatabase> \\ ...}{
    + registerPipelineElement(InvocableStreamPipesEntity) : void \\ + unregisterPipelineElement(String) : void \\ + run() : void \\ ...
    }
    
    \umlinherit[geometry=-|]{CheckpointingWorker}{Runnable}
    \umlimpl[geometry=-|-]{BackendCheckpointingWorker}{CheckpointingWorker}
    \umlimpl[geometry=-|-]{ContainerCheckpointingWorker}{CheckpointingWorker}
    
    \end{tikzpicture}
    \caption{Abbreviated UML class diagram for the \textit{Container/BackendCheckpointingWorker}}
    \label{fCheckpointingWorkerUML}
\end{figure}



The database setup is used as storage for \gls{pe} state checkpoints. When a pipeline is started, two \textit{PipelineElementDatabases} are created for each \gls{pe} in the pipeline. The backend creates the first one at the central-level. The second one is created in the invoked \gls{pe}'s container at the \gls{pe}-level. In addition to creating these instances, each \textit{PipelineElementDatabase} and its respective \gls{pe} are registered for checkpointing in the container’s specific \textit{CheckpointingWorker} (depicted in Figure \ref{fCheckpointingWorkerUML}). The \textit{CheckpointingWorker} oversees and executes the asynchronous checkpointing. All registered \gls{pe}s are periodically checkpointed until they are unregistered from the \textit{CheckpointingWorker}. The \textit{CheckpointingWorker} has distinct implementations for the backend (central-level) and for the containers containing the \gls{pe}s (\gls{pe}-level). The \gls{pe}-level \textit{CheckpointingWorker} uses the \gls{pe}'s "getState" method to get a checkpoint, which is then saved in the \gls{pe}'s \textit{PipelineElementDatabase}. The central-level \textit{CheckpointingWorker} calls the \gls{pe}'s "getCheckpoint" method through its \gls{rest} endpoint. This method returns the most recent checkpoint from the \textit{PipelineElementDatabase} at \gls{pe}-level.\\
\gls{dlac} is realized as the composition of a set of \textit{ContainerCheckpointingWorkers} at \gls{pe}-level and one \textit{BackendCheckpointingWorker} at central-level. Both of the \textit{CheckpointingWorker} implementations are optimized to idle for as much time as possible between checkpointing registered \gls{pe}s, to minimize the impact on computing resources.\par





\subsection{Migration Procedure}
\label{lImplementationMigrationProcedure}
The aforementioned modifications to the \gls{pe}s need to be made accessible on the \gls{pe} side to implement the conceptual migration framework. Additionally, the procedures for the \textit{\acrshort{pe} live migration} and the \textit{\acrshort{pe} restoration} need to be implemented in the backend.\par

\subsubsection{Pipeline Element-Side Implementation}
\label{lOperatorSideImplementation}
To make the modifications and extensions introduced in \ref{lModifications} accessible, the \gls{pe}'s \gls{rest} interface is extended. The newly implemented and modified methods, which are accessible through individual \gls{rest} endpoints, are listed in Table \ref{tNewMethods}.


\begin{table}[H]
    \caption{Overview of methods added on the \gls{pe}-side}
    \label{tNewMethods}
    \begin{tabular}{|p{5cm}|p{10cm}|}
    \hline
     \textbf{Method}&\textbf{Description}\\ 
     \hline\hline
     pauseElement(String, String)&Pauses the consumption of events from the message broker until the \gls{pe} is resumed or detached. Event processing is paused through the "pause" method added to the \textit{SpKafkaConsumer} class.\\
     \hline
     resumeElement(String, String)&Resumes the \gls{pe} if it has been paused before. Event processing is resumed through the "resume" method added to the \textit{SpKafkaConsumer} class.\\
     \hline
     getState(String, String)&Returns the current operator state. When executed, this method pauses event consumption, gets a representation of the operator state, using the aforementioned functionalities to serialize the buffer and the processing state, and resumes event processing immediately after that.\\
     \hline
     getCheckpoint(String, String)&Returns the most recent operator state saved in the \gls{pe}s state-database. This method relies on the \gls{pe}-side state database, where the \gls{pe}'s checkpoints are regularly saved to by the \textit{ContainerCheckpointingWorker}.\\
     \hline
     invokeRuntime(String, String)&Modified to recreate the \gls{pe} state from a \gls{pe} state checkpoint before event processing begins. The processing and buffer states are restored using the functions and procedures described in section \ref{lModifications}.\\
     \hline
    \end{tabular}
\end{table}

\subsubsection{Backend-Side Implementation}
\label{lBackendSideImplementation}
On the backend side, the \textit{\acrshort{pe} live migration} and the \textit{\acrshort{pe} restoration} are implemented.\par

\textbf{\acrlong{pe} Live Migration}\par
Figure \ref{fOSMImplementation} provides a shortened and simplified representation of the \textit{\acrshort{pe} live migration} implementation, realizing the concept outlined in \ref{lMigrationProceedure}. The depicted "migrate" method is implemented into the \textit{PipelineExecutor} class and is accessible through a \gls{rest} endpoint in the backend.\par




\begin{figure}[H]
    \begin{lstlisting}[language=Java]
public PipelineOperationStatus migrate(PipelineElement pe, String targetNodeUrl){
    Pipeline pipeline = PipelineStorage.get(pe.getPipelineID());
    String originNodeUrl = pe.getUrl();
    BackendCheckpointingWorker.unregisterPipelineElement(pe.ID)
    //Pause Pipeline Element that should be migrated
    PipelineOperationStatus status = HttpRequest.pause(pe);
    if(!status.isSuccess()){
      status.setTitle("Could not pause the Pipeline Elements.");
      BackendCheckpointingWorker.registerPipelineElement(pe)
      return status;
    }
    //Get state of the Pipeline Elements
    PipelineElementState state = HttpRequest.getState(pe);
    //Start Pipeline Element with received states
    pe.setUrl(targetNodeUrl);
    PipelineOperationStatus statusInvoc = HttpRequest.invoke(pe, state);
    if(statusInvoc.isSuccess()){
      //Discard Pipeline Element on origin node
      operator.setUrl(originNodeUrl);
      PipelineOperationStatus detach = HttpRequest.detach(pe);
      if(!detach.isSuccess()){
        statusInvoc.setTitle("Failed to detach old Elements.");
        statusInvoc.setSuccess(false);
      }
      pipeline.getPipelineElement(pe).setUrl(targetNodeUrl);
      pe.setUrl(targetNodeUrl);
      BackendCheckpointingWorker.registerPipelineElement(pe);
      return statusInvoc;
    }else{
      //Resume processing on origin node
      operator.setUrl(originNodeUrl);
      PipelineOperationStatus statusRes = HttpRequest.resume(pe);
      if(statusRes.isSuccess()){
        statusRes.setSuccess(false);
        statusRes.setTitle("Migration unsuccessful, resumed on origin");
      }else{
        statusRes.setTitle("Migration unsuccessful, could not resume.");
      }
      BackendCheckpointingWorker.registerPipelineElement(pe);
      return statusRes;
    }
}
    \end{lstlisting}
    \caption{Abbreviated and simplified implementation of the \textit{\acrshort{pe} live migration}.}
    \label{fOSMImplementation}
\end{figure}






In addition to what is outlined in the concept conceptual framework, implementation related factors are addressed. First, the correct registration to the \textit{CheckpointingWorker} is handled. Second, the stored pipeline related metadata is adjusted. This metadata contains, amongst other information, a topological description of the pipeline. Since this information changes during a successful migration, the saved pipeline representation is modified as well. Furthermore, the implemented method returns a \textit{PipelineOperationStatus} instance, which provides information about the migration outcome. This information can be used elsewhere to handle the outcome and to react to it.\\
Looking at the implementation in detail, the code can be divided into five blocks. From lines 2-4, the procedure is prepared with its necessary preceding actions. Next, in lines 6-11, the \gls{pe} on the origin node is paused. If this is unsuccessful, the procedure is aborted. In the third block, the current \gls{pe} state is fetched from the origin node (line 13). After that, lines 15 and 16 invoke the \gls{pe} with the received \gls{pe} state on the target node. Lastly, lines 17-41 handle the outcome of the invocation. If it was successful, the \gls{pe} on the origin node is discarded (lines 19-28). If it was unsuccessful, the \gls{pe} on the origin node is resumed, and a failure notice is returned (lines 31-40).\\
This implementation aims at achieving an exactly-once semantic throughout the migration. By leveraging the Kafka offset management for the buffer state representation, the target node \gls{pe} can connect to exactly the position in the event stream, at which processing was suspended on the origin node. Pausing the \gls{pe} on the origin node in a controlled manner guarantees that the consumer offset corresponds with the latest processed event.\par



\textbf{\acrlong{pe} Restoration}\par

Figure \ref{fOSRImplementation} shows a shortened and simplified illustration of the \textit{\acrshort{pe} restoration} implementation. The \textit{\acrshort{pe} restoration} procedure is implemented in the \textit{PipelineExecutor} class and accessible through its own \gls{rest} endpoint.\par

Similarly to the implementation of the \textit{\acrshort{pe} live migration}, the procedure handles the \gls{pe} checkpointing registration, and the modification of the saved pipeline description.\\
The implemented procedure can be partitioned into four code blocks. In lines 2-4, necessary preparations for the procedure are taken. Next, the most recent state checkpoint is fetched from the centralized state database in line 6. In lines 7 and 8, the saved pipeline representation is modified, and the \gls{pe}'s \gls{url} is adjusted to match the target node's \gls{url}. Finally, the \gls{pe} is invoked from the checkpoint on the target node (lines 10-12). After the invocation, the invocation's status is returned.\\

\begin{figure}[H]
    \begin{lstlisting}[language=Java]
public PipelineOperationStatus restore(PipelineElement pe, String targetNodeUrl){
    Pipeline pipeline = PipelineStorage.get(pe.getPipelineID());
    String originNodeUrl = pe.getUrl();
    BackendCheckpointingWorker.unregisterPipelineElement(pe.ID)
    //Get state from database
    PipelineElementState state = DatabasesSingleton.INSTANCE.getDatabase(pe.ID).getLast();
    pe.setUrl(targetNodeUrl);
    pipeline.getPipelineElement(pe).setUrl(targetNodeUrl);
    //Start Pipeline Element with state
    PipelineOperationStatus  statusInvoc = HttpRequest.invoke(pe, state);
    BackendCheckpointingWorker.registerPipelineElement(pe);
    return status;
}
    \end{lstlisting}
    \caption{Abbreviated and simplified implementation of the \textit{\acrshort{pe} restoration}.}
    \label{fOSRImplementation}
\end{figure}


The implementation of the \textit{\acrshort{pe} restoration} cannot guarantee an exactly-once semantic throughout the migration. Instead, it provides an at-least-once semantic. An at-least-once semantic guarantees that no event stays unprocessed \cite{Huang.2001}. However, it does not guarantee that an event is not processed twice \cite{Huang.2001}. In the implementation at hand, an event could be processed a second time because the consumer offset tracked by Kafka is only adjusted when an event is committed. Committing an event refers to sending an acknowledgment that an event has been processed to the Kafka broker. If a \gls{pe} is interrupted after successfully processing an event but before committing this event, the \textit{\acrshort{pe} restoration} does not achieve an exactly-once semantic throughout the migration. In such a case, all uncommitted events are fully processed again.\par